1. Script to load the data
    a. Change the work directory to punchh
    b. Copy the email.json.gz under data/
    c. docker-compose build
    d. docker-compose up
    Step c will automatically run the script present under punchh/app/data_loader.py

2. Table(s) design 

    Since I am working with only email dataset, there is only one table to hold all the data. The table schema is present under punchh/db/init.sql

3. Assumptions

    a. The data loading script assumes that the input data contains one record per line, each record being a json record.
    b. The data loading script also assumes that all the data is present in just one file. To process multiple files, code changes are required.
    c. The data loading script assumes that a record could be malformed or individual values could be incorrect.
       Therefore validation is performed and all invalid entries are written to /data/error_records.txt


4: SQL

    4.1 SQL to Get emails of users who opened email
    Answer:
            select
                distinct(email)
            from
            email_campaigns
            where event='open';

    4.2 No. of users who opened emails between 1-2 days, 2-5 days and > 5 days after email is sent. Ex : If the email is sent 21st, # of users who opened email between 23rd and 26th including both 23rd and 26th
    Answer:
            select
                count(distinct(email)),
                case
                    when (email_timestamp::date - campaign_date) < 2  then 'less_than_2 days'
                    when (email_timestamp::date - campaign_date) >= 2 and (email_timestamp::date - campaign_date) <= 5  then 'between 2 and 5 days'
                    else '5 or more days'
                end as days_interval
            from email_campaigns
            where event='open'
            group by days_interval;


    4.3 Identify the campaign which is more successful ? ( define what you think success means and write sql) 

    Answer:
        a. If we define "success" by number of users who opened our email, then the campaign with most emails opened is the campaign that is most successful

            select
                count(distinct(email)),
                campaign_name from email_campaigns
            where event='open'
            group by campaign_name
            order by count desc

        b. Success can also be calculated as ratio of number of emails opened divided by number of emails delivered

            select
                (total_opened / total_delivered ) as opened_percentage,
                campaign_name
                from
                    (
                    select
                        cast(sum(case when event='open' then 1 else 0 end) as decimal)  as total_opened,
                        cast(sum(case when  event='delivered' then 1 else 0 end) as decimal) as total_delivered,
                        campaign_name from email_campaigns group by campaign_name
                    ) as totals
            where
            total_delivered > 0
            order by opened_percentage desc;


        c. However, if we also factor in number of people who unsubscribed, then the campaign with max(email opened - unsubscribed) is the most successful

                select
                    (total_opened - total_unsubscribed) as net_gains,
                    campaign_name
                from
                    (
                    select
                        cast(sum(case when event='open' then 1 else 0 end) as decimal) as total_opened,
                        cast(sum(case when event='unsubscribe' then 1 else 0 end) as decimal) as total_unsubscribed,
                        campaign_name
                    from
                    email_campaigns
                    group by campaign_name
                    ) as totals
                order by net_gains desc;

        d. A campaign can also be defined successful by the average time people took to open the email after it was sent.

            select
                avg(email_timestamp::date - campaign_date) as time_to_open,
                campaign_name from email_campaigns
            group by campaign_name
            order by time_to_open desc;


5. Dockerize the solution:

    The solution is dockerized.

6. Any test cases you can think of ?

    1. We can add invalid records to the file. For instance
        a. Malformed Json
        b. Record with invalid date ex: 2020-13-13
        c. Record where timestamp is zero or less
        d. Record where email, campaign_name, event_id, message_id, event are not strings or empty.
        e. Record where business_id is not a number.
      In all such instances, the row will be written to /data/error_records.txt with the correct reason(s).

    2. The data can have different email_timestamp records for when the email was opened. In this case the
       query in 4.2 should give different numbers as per the buckets.

    3. We can also test against the definition of "successful" campaign as defined in 4.3.

    4. Unit tests have been omitted due ot lack of time. But I would definitely add lots of them.

7) How would you design it if it was streaming data ?

   If this was streaming data then the following changes could have been made:

   a. Instead of reading the entire file at a time, the code would be reading the data in micro-batches.
   b. To sustain the load on the database, an intermediate storage that supports a higher data throughput (a message queue) could be used.
   c. Multiprocessing could also be added to increase data load paralellization

8) Any other insights you can derive from the data 

   - The campaign with most "opened" emails may not be the most successful campaign.
   - A campaign with lower number of opened email could be more successful because it has a higher delivered:opened ration.
   - The email goes through multiple stages before reaching the customer
        "send" -> "deferred"
        "send" -> "processed"
        "processed" -> "bounce"
        "processed" -> "dropped"
        "processed" -> "delivered"
        "delivered" -> "clicked"
        "delivered" -> "spam report"
        "spam report" -> "clicked"
        "clicked" -> "open"
        "open" -> "unsubscribe"

   - In the sample input email.json.gz, all emails were opened the same day as they were sent.


9. If I had more time, then I would have made the following improvements
    a. Add unit tests
    b. Use a decorator to encapsulate database connection
    c. Use cerberus Json schema validator (But it can affect the speed of load script)